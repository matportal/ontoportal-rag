# ONTO-RAG-V1: GenAI-Powered RAG System for Ontologies

This project implements a sophisticated, production-grade Retrieval-Augmented Generation (RAG) system designed to make complex ontologies accessible through natural language queries. It serves as a foundational platform for any application requiring natural language interaction with ontological knowledge.

## Project Vision

The primary goal of this system is to transform dense, machine-readable ontologies (like those in OWL/RDF format) into a queryable knowledge base. Users can ask complex questions about the concepts, relationships, and hierarchies within a given ontology and receive accurate, context-aware answers generated by a Large Language Model (LLM).

## High-Level Architecture

The system is divided into two main pipelines:

1.  **Asynchronous Ingestion Pipeline:** Converts raw ontology files into a searchable index in Weaviate.
    -   `Ontology File -> [pylode] -> Markdown -> [langchain] -> Chunks -> [Weaviate] -> Indexed Knowledge`

2.  **Synchronous Retrieval Pipeline:** Answers user queries in real-time.
    -   `Query -> [Weaviate Hybrid Search] -> Candidates -> [Cohere Rerank] -> Top Chunks -> [LLM] -> Answer`

A more detailed explanation of the architecture can be found in the `docs/ARCHITECTURE.md` file.

---

## Getting Started

You can run this project using either Docker (recommended for a full-stack, isolated environment) or locally for development.

### Prerequisites

- Docker Engine plus the `docker compose` plugin.
- For local-only development: `python3.10+`, `pip`, Redis, and Weaviate running on the configured ports.
- API keys for your LLM, reranker, and embeddings providers.

### 1. Configuration

Environment variables are sourced from a `.env` file (ignored by git). Create it from the provided template and add your credentials (never commit them):

```bash
cp app.env.example .env
```

If you proxy OpenAI-compatible calls through LiteLLM or similar, set `OPENAI_BASE_URL` and `OPENAI_EMBEDDINGS_BASE_URL` to `http://host.docker.internal:4000/v1` so containers can reach the host service.

**ROBOT (optional).**  
If you would like the ingestion pipeline to auto-repair problematic ontologies, download the [ROBOT](https://robot.obolibrary.org/) CLI (`robot.jar`), set `ROBOT_ENABLED=true`, and point `ROBOT_JAR_PATH` to the jar. The worker will run `robot convert`/`robot repair` before parsing, which resolves many malformed RDF/XML and Turtle issues.

### 2. Running with Docker (Recommended)

This is the easiest way to get the entire application stack (API, worker, Weaviate, Redis) running.

```bash
# Build and start all services in detached mode
docker compose up --build -d
```

- The API will be available at `http://localhost:8000`.
- The API documentation (Swagger UI) will be at `http://localhost:8000/docs`.

To view logs for all services:
```bash
docker compose logs -f
```

To stop the services:
```bash
docker compose down
```

### 3. Local Development (Without Docker)

If you prefer to run the Python services directly on your host machine, you can use the provided script.

**Note:** You must have Redis and Weaviate running and accessible on the ports configured in your `.env` file (defaults to `localhost:6379` and `localhost:8080`).

```bash
# Run the local development script
./scripts/run_local.sh
```

This will:
1. Create a Python virtual environment in `./venv`.
2. Install all required dependencies.
3. Start the FastAPI server and the Celery worker.

Press `Ctrl+C` to shut down the services.

### 4. Running Tests

To ensure everything is working correctly, you can run the test suite. The tests are designed to run in isolation and will mock external services.

```bash
# Run the test script
./scripts/run_tests.sh
```

This will install all dependencies and run `pytest` with coverage reporting.

---

## Usage

See the `tutorials/` directory for step-by-step guides on how to use the API for ingestion and querying.
